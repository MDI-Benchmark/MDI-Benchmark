# Multi-Dimensional Insights:  Benchmarking Real-World Personalization in Large Multimodal Models

![Dataset](https://img.shields.io/badge/Dataset-MDI--Benchmark-green)
![Multi-Modal](https://img.shields.io/badge/Task-Multi--Modal-red)
![Mathematical Reasoning](https://img.shields.io/badge/Task-Real_world_Reasoning-red) 

<p align="center">
    <img src="./assets/icon.png" width="35%"> <br>
</p>

## Outlines
- [ğŸ’¥ News ğŸ’¥](README.md#-news-)
- [ğŸ‘€ About MDI-Benchmark]()
- [ğŸ† Leaderboard on MDI-Benchmark ğŸ†](https://github.com/MDI-Benchmark/MDI-Benchmark/blob/main/README.md#-leaderboard-on-MDI-Benchmark-)
<!-- - [ğŸ“ Evaluation Piplines on MDI-Benchmark](https://github.com/MDI-Benchmark/MDI-Benchmark/blob/main/README.md#-evaluation-piplines-on-MDI-Benchmark) -->
- [ğŸ“Š MDI-Benchmark Dataset](https://github.com/MDI-Benchmark/MDI-Benchmark/blob/main/README.md#-MDI-Benchmark-dataset)
- [ğŸ“œ License](https://github.com/MDI-Benchmark/MDI-Benchmark/blob/main/README.md#-license)
- [ğŸ¤ Contributors](https://github.com/MDI-Benchmark/MDI-Benchmark/blob/main/README.md#-contributors)

## ğŸ’¥ News ğŸ’¥
  **[2023.12.18]** Our paper is now accessible at https://arxiv.org/abs/2412.12606.

  **[2024.12.18]** Our project homepage can be accessed at https://MDI-Benchmark.github.io/.

  **[2024.12.18]** Our dataset will be released as open source soon.

## ğŸ‘€ About MDI-Benchmark
To align with the <b>actual needs of humans</b> for Large Multimodal Models, we propose a multi-modal benchmark for providing a thorough assessment of the capacities of LMMs in practical, real-world scenarios.

<p align="center">
    <img src="./assets/fig1.png" alt="Overview of MDI-Benchmark" style="width: 95%;"> <br>
    The overview of the <b>MDI-Benchmark</b> six real-world multimodal scenarios.
</p>

The MDI-Benchmark includes over <b>500 real-world images</b> and <b>1.2k human-posed questions</b>, spanning six real-world multimodal scenarios. Each scenario is divided into 3 sub-domains with 2 levels of complexity. Additionally, we incorporate age factors into the evaluation to guide LMMs in personalizing their responses for different demographic groups.

<p align="center">
    <img src="./assets/fig2.png" alt="MDI-Benchmark question" style="width: 95%;"> <br>
    The <b>MDI-Benchmark</b> includes real needs of different age groups in six major real-world scenarios.
</p>

With the MDI-Benchmark, we conduct a comprehensive evaluation of several mainstream LMMs. Specifically, GPT-4o achieved the best results across all indicators, but there is still significant room for improvement in addressing the needs of different age groups. Further analysis across dimensions such as Scenario, Complexity and Age provides valuable insights for developing reliable, personalized human assistants.

<p align="center">
    <img src="./assets/leaderboard.png" alt="leaderboard" style="width: 95%;"> <br>
    Performance of the model at <b>different difficulty levels</b> and the overall performance results of the model under the score metric.
</p>

We hope our research will advance the application of multimodal large models in real-world scenarios and pave the way for the development of <b>multi-dimensional personalization</b>.




## ğŸ† Leaderboard on MDI-Benchmark ğŸ†
ğŸš¨ğŸš¨ The [Leaderboard](https://MDI-Benchmark.github.io/#leaderboard) is continuously being updated. We welcome the results of your model!
To submit your results to the leaderboard on the **testmini** subset, please send to [this email](mailto:zhangyifan_ai@bupt.edu.cn) with your result JSON file and score CSV file.

<!-- ## ğŸ“ Evaluation Piplines on MDI-Benchmark

### Response Generation 
The models generate responses based on the given questions and images. Examples for generating responses from some LMMs are provided in the [evaluation](./evaluation). Our prompt specifies the format of answer generation to facilitate subsequent extraction of the answer using string matching. Please refer to the following template to prepare your result JSON files for subsequent evaluation.
```json
{
        "question number": 1,
        "split": "testmini",
        "key": "level1-architecture-old",
        "image_path": "architecture/houseplaning_0",
        "question": "Looking at this house plan, how many bedrooms does this house have?",
        "option": "A.1; B.2; C.3; D.4",
        "answer": "C",
        "response": "<Thought process>: ... <Answer>: ..."
}


```
### Score Calculation
Due to the multiple-choice question format of our dataset and the specific answer generation prompt, we use string matching to directly extract answers, which eliminates the high cost of using additional models for further answer extraction.  The extracted answer is normalized to an option letter and calculate scores on our proposed four-dimensional metrics in [accuracy.py](https://github.com/MDI-Benchmark/MDI-Benchmark/blob/main/evaluation/accuracy.py).


```sh
cd evaluation

python accuracy.py \
--model_name GPT-4o \
--output_json ../output/GPT-4o.json  \
--main_results_csv_path ../result/res.csv
``` -->

## ğŸ“Š MDI-Benchmark Dataset

In contrast to existing work, MDI-Benchmark emphasizes the model's performance on real-world problems across various ages and complexities within specific task scenarios, it is structured along three different dimensions: scenario, age, and problem complexity.

### Exmaples
<details>
<summary>ğŸ”Examples of question samples.</summary>
<p align="center">
    <img src="./assets/trans.png" width="90%"> <br>
    Examples of Transport Scenario.
</p>


<p align="center">
	<img src="./assets/edu_level.png" width="90%"> <br>
	Examples of Education Scenario Level Questions.
</p>

<p align="center">
	<img src="./assets/social_age.png" width="90%"> <br>
	Example of Social Service Scenario Age Questions.
</p>

</details>



## ğŸ“œ License

Our dataset are distributed under the [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) license.


## :white_check_mark: Cite

If you find **MDI-Benchmark** useful for your your research and applications, please kindly cite using this BibTeX:

```bibtex
@misc{zhang2024multi,
      title={Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models}, 
      author={YiFan Zhang and Shanglin Lei and Runqi Qiao and Zhuoma GongQue and Xiaoshuai Song and Guanting Dong and Qiuna Tan and Zhe Wei and Peiqing Yang and Ye Tian and Yadong Xue and Xiaofei Wang and Honggang Zhang},
      year={2024},
      eprint={2412.12606},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.12606}, 
}
```


## ğŸ¤ Contributors
Here are the key contributors to this project:

Yifan Zhang, Shanglin Lei, Runqi Qiao, Zhuoma GongQue, Xiaoshuai Song, Guanting Dong, Qiuna Tan, Zhe Wei, Peiqing Yang, Ye Tian, Xiaofei Wang, Honggang Zhang
